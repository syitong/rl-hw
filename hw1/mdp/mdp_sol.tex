\noindent{Question 5}
\begin{enumerate}[(a)]
    \item No report is required for this question.
    \item Uniformly random policy evaluation:
    \begin{minted}{python}
[[ 2.64  8.54  3.97  5.09  0.94]
 [ 0.98  2.6   1.87  1.57  0.11]
 [-0.36  0.43  0.4   0.09 -0.75]
 [-1.31 -0.67 -0.56 -0.8  -1.48]
 [-2.23 -1.62 -1.48 -1.68 -2.32]]
    \end{minted}
    \item Optimal value function and optimal policy after policy iteration:
    \begin{minted}{python}
[[18.88 21.5  18.88 16.5  14.49]
 [16.58 18.88 16.58 14.56 12.78]
 [14.56 16.58 14.56 12.78 11.22]
 [12.78 14.56 12.78 11.22  9.85]
 [11.22 12.78 11.22  9.85  8.65]]

  e    nesw   w    nesw   w

  e     n     nw    w     w

  e     n     nw    nw    nw

  e     n     nw    nw    nw

  e     n     nw    nw    nw
  \end{minted}
If in-place update is used, the return will be
  \begin{minted}{python}
  e    nesw   w    nesw   w

  e     n     nw    w     w

  e     n     nw    w     w

  e     n     nw    w     w

  e     n     nw    w     w
    \end{minted}
    \item optimal value function and the optimal policy after value iteration
    \begin{minted}{python}
[[18.88 21.5  18.88 16.5  14.49]
 [16.58 18.88 16.58 14.56 12.78]
 [14.56 16.58 14.56 12.78 11.22]
 [12.78 14.56 12.78 11.22  9.85]
 [11.22 12.78 11.22  9.85  8.65]]

  e    nesw   w    nesw   w

  e     n     nw    w     w

  e     n     nw    nw    nw

  e     n     nw    nw    nw

  e     n     nw    nw    nw
    \end{minted}
    \item For episodic version, when gamma equals 1, optimal value function
    after value iteration:
    \begin{minted}{python}
[[10. 10. 10.  5. 10.]
 [10. 10. 10. 10. 10.]
 [10. 10. 10.  0. 10.]
 [10. 10. 10. 10. 10.]
 [10.  0. 10. 10. 10.]]
    \end{minted}
    When gamma equals 0.9, optimal value function after value iteration:
    \begin{minted}{python}
[[ 8.78 10.    8.78  5.    5.22]
 [ 7.71  8.78  7.71  6.77  5.94]
 [ 6.77  7.71  6.77  0.    5.22]
 [ 5.94  6.77  5.94  5.22  4.58]
 [ 5.22  0.    5.22  4.58  4.02]]
    \end{minted}
    When gamma equals 0.8, optimal value function after value iteration:
    \begin{minted}{python}
[[ 7.62 10.    7.62  5.    3.81]
 [ 5.8   7.62  5.8   4.42  3.37]
 [ 4.42  5.8   4.42  0.    2.57]
 [ 3.37  4.42  3.37  2.57  1.96]
 [ 2.57  0.    2.57  1.96  1.49]]
    \end{minted}
    We can see that when gamma equals 1 and 0.9, we cannot achieve our intention. For example, in both
    cases, the agent at the top right corner will move to the west and only get a suboptimal reward.
    When gamma equals 0.8, we can achieve out intention.
    \item
    We can see that for the agent to learn the optimal policy, a discount factor less than 1 is necessary,
    but it cannot be too small. If it is too small, the reward from A will be discounted too much and
    the agent may be prefer to move to B if it is closer. Actually the location that will be first affected
    when gamma decreases it the top right corner. The value of B is always 5, while the value of the
    location 9 is
    \[
        10\left(\frac{0.8\gamma}{1-0.2\gamma}\right)^4\,.
    \]
    This is the result of the Bellman's equation in our case,
    \[
        v^*(s) = 0.8\gamma v^*(s') + 0.2\gamma v^*(s)\,,
    \]
    where $ s' $ is the destination of the optimal move.
    By setting
    \[
        10\left(\frac{0.8\gamma}{1-0.2\gamma}\right)^4 = 5\,,
    \]
    we get $ \gamma \approx 0.869 $.
\end{enumerate}
