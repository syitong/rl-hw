{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# import RidiculusTaxi\n",
    "import mytaxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#state:501, #action6\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3').unwrapped\n",
    "nS = env.nS\n",
    "nA = env.nA\n",
    "print(\"#state:{}, #action{}\".format(nS, nA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_argmax(Q_s):\n",
    "    qmax = np.max(Q_s)\n",
    "    actions = []\n",
    "    for i,q in enumerate(Q_s):\n",
    "        if q == qmax:\n",
    "            actions.append(i)\n",
    "    return actions\n",
    "\n",
    "# def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "#     def policy_fn(observation):\n",
    "#         A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "# #         best_action = np.argmax(Q[observation])\n",
    "#         best_actions = my_argmax(Q[observation])\n",
    "#         A[best_actions] += (1.0 - epsilon) / len(best_actions)\n",
    "#         # A = A/sum(A)\n",
    "#         return A\n",
    "#     return policy_fn\n",
    "def eps_policy(Q_s, epsilon=0.1, nA=nA):\n",
    "    A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "    actions = my_argmax(Q_s)\n",
    "    A[actions] += (1.0 - epsilon) / len(actions)\n",
    "    probs = A / sum(A)\n",
    "    action = np.random.choice(nA, p=probs)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, runs, num_episodes, discount_factor=1.0, epsilon=0.1, alpha=0.9):\n",
    "    \n",
    "    nA = env.nA\n",
    "    nS = env.nS\n",
    "    \n",
    "    rew_alloc = []\n",
    "    for run in range(runs):\n",
    "        Q = np.zeros((nS,nA))\n",
    "        rew_list = np.zeros(num_episodes)\n",
    "        returns_sum = defaultdict(float)\n",
    "        returns_count = defaultdict(float)\n",
    "        for i_episode in range(num_episodes):\n",
    "            # Generate an episode.\n",
    "            # An episode is an array of (state, action, reward) tuples\n",
    "            episode = []\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            counter = 0\n",
    "            while not done:\n",
    "    #        for t in range(100):\n",
    "                counter += 1\n",
    "                print('\\rEpisode {}/{} Step {}     '.format(i_episode,num_episodes,counter), end='')\n",
    "                sys.stdout.flush()\n",
    "                action = eps_policy(Q[state], nA=nA)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                rew_list[i_episode] += reward\n",
    "                episode.append((state, action, reward))\n",
    "                if done:\n",
    "                    break\n",
    "                state = next_state\n",
    "\n",
    "            # Find all (state, action) pairs we've visited in this episode\n",
    "            sa_in_episode = set([(x[0], x[1]) for x in episode])\n",
    "            for state, action in sa_in_episode:\n",
    "                sa_pair = (state, action)\n",
    "                # Find the first occurance of the (state, action) pair in the episode\n",
    "                first_occurence_idx = next(i for i,x in enumerate(episode)\n",
    "                                           if x[0] == state and x[1] == action)\n",
    "                # Sum up all rewards since the first occurance\n",
    "                G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "                # Calculate average return for this state over all sampled episodes\n",
    "                returns_sum[sa_pair] = alpha*returns_sum[sa_pair] + G\n",
    "                returns_count[sa_pair] += 1.0\n",
    "                # The policy is improved implicitly by changing the Q dictionary\n",
    "                Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "        rew_alloc.append(rew_list)\n",
    "    rew_list = np.mean(np.array(rew_alloc),axis=0)\n",
    "    fig = plt.figure()\n",
    "    plt.plot(rew_list)\n",
    "    plt.savefig('mc_control-interim.eps')\n",
    "    plt.close(fig)\n",
    "    return Q, rew_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_function(V, baseline, title=\"Value Function\"):\n",
    "    \"\"\"\n",
    "    Plots the value function as a surface plot.\n",
    "    \"\"\"\n",
    "    V_ordered = OrderedDict(sorted(V.items()))\n",
    "    \n",
    "    print('\\n')\n",
    "    print(len(V.keys()))\n",
    "    \n",
    "    v_s = np.zeros(len(V.keys()))\n",
    "    idx = 0\n",
    "    for key, val in V_ordered.items():\n",
    "        v_s[idx] = val\n",
    "        idx +=1\n",
    "\n",
    "    # print(np.sort(np.asarray(V.keys())))\n",
    "\n",
    "#     plt.plot(np.asarray(v_s), marker='o',linewidth=2)\n",
    "    plt.plot(v_s,marker='o',linestyle='None',label='mc')\n",
    "    plt.plot(baseline,marker='x',linestyle='None',label='base')\n",
    "    plt.legend([\"MC\", \"Baseline\"])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"State\", fontsize=20)\n",
    "    plt.savefig(\"MC_control.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting: Create value function from action-value function\n",
    "# by picking the best action at each state\n",
    "# V = defaultdict(float)\n",
    "# for state, actions in Q.items():\n",
    "#     action_value = np.max(actions)\n",
    "#     V[state] = action_value\n",
    "# baseline = np.load('baseline.npy')\n",
    "# plot_value_function(V, baseline, title=\"MC_control\")\n",
    "# # plot_Q_table()\n",
    "# with open('qtable_mc','w') as fp:\n",
    "#     fp.write(str(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 546/2500 Step 41      "
     ]
    }
   ],
   "source": [
    "Q, rew_list = mc_control_epsilon_greedy(env, runs=1, num_episodes=2500, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
