%!TEX root = ./homework1.tex

\noindent{Question 5} [35 pts] (Variant of Example 3.5 on p60)
Consider the 5 by 5 gridworld representation of
a simple finite MDP. At each cell, four actions are possible: north, south, east
and west. When the agent takes an action, there is a probability of 0.2 for it
to slip and leave its location unchanged, otherwise it moves in the correct
direction by one cell. Any actions that would take the agent off the grid
also leave the agent's location unchanged and result in a reward of -1.
Other actions result in a reward of 0, except those that move the agent out
of the special state A and B. From state A, all four actions yield a reward of
+10 and take the agent to A' deterministically. From state B, all four actions
yield a reward of +5 and take the agent to B' deterministically.
\begin{center}
    \begin{tabular}{ccccc}
      o & A  & o & B  & o\\
      o & o  & o & o  & o\\
      o & o  & o & B' & o\\
      o & o  & o & o  & o\\
      o & A' & o & o  & o\\
    \end{tabular}
\end{center}
The 25 states in the grid will be identified as the integers
from $ 0 $ to $ 24 $ in the order of left to right and top to bottom.
The four actions are encoded by north=0, east=1, south=2, west=3.

Now please complete the programming problems below. Remember to always comply with
the following rules!!
\begin{itemize}
  \item Development environment: \texttt{python 3.5+}.
  \item Allowed non-standard packages: \texttt{numpy}.
  \item Your functions must follow the definitions given in the
  problem with correct default values passed.
  \item Put all your functions in one \texttt{.py} file named after your UM
  uniquename. Make sure it can be imported and all the functions can be
  called.
\end{itemize}

\begin{enumerate}
    \item Work out the transition probability and rewards for the MDP described
    above using the function defined as:
    \begin{minted}{python}
def gridworld(slip_prob=0.2):
    # slip_prob is the probability the agent slips.
    return P
    '''
    P = {
            s1: {a1: [(p(s'_1|s1,a1), s'_1, reward(s'_1,s1,a1)),
                      (p(s'_2|s1,a1), s'_2, reward(s'_2,s1,a1)),
                      ...
                     ],
                 a2: ...,
                 ...
                 },
            s2: ...,
            ...
        }
    '''
    \end{minted}
    Only non-zero transition probabilities need to be considered. In other
    words, \texttt{len(P[s][a])} is at most 4.

    \item{}[7 pts] Assume that the agent follows the uniformly random policy, that is,
    takes one of four actions with equal probability at all states.
    Implement the policy evaluation algorithm and compute the value
    function for this uniformly random policy.
    The function should be defined as below:
    \begin{minted}{python}
def policy_eval(P, policy=uniform_policy, theta=0.0001, gamma=0.9):
    return V
    '''
        P: as returned by your gridworld(slip=0.2).
        policy: probability distribution over actions for each states.
          Default to uniform policy.
        theta: stopping condition.
        gamma: the discount factor.
        V: 5 by 5 numpy array where each entry is the value of the
          corresponding location. Initialize V with zeros.
    '''
    \end{minted}
    Other than the code, please put your value function with 2 decimals
    in your solution to the homework as follows,
    \begin{minted}{python}
[[14.81 17.63 14.81 12.91 10.85]
 [12.45 14.81 12.45 10.85  9.12]
 [10.46 12.45 10.46  9.12  7.66]
 [ 8.79 10.46  8.79  7.66  6.44]
 [ 7.38  8.79  7.38  6.44  5.41]]
    \end{minted}

    \item{}[7 pts] Implement the policy iteration algorithm in the following
    function to find the optimal policy.
    \begin{minted}{python}
def policy_iter(P, theta=0.0001, gamma=0.9):
    return V, policy
    '''
        policy: 25 by 4 numpy array where each row is a probability
          distribution over moves for a state. If it is
          deterministic, then the probability will be a one hot vector.
          If there is a tie between two actions, break the tie with
          equal probabilities.
          Initialize the policy with zeros.
    '''
    \end{minted}
    Other than the code, please put your value function with 2 decimals
    in your solution. And also visualize the optimal policy in
    the grid in your solution as follows,
    \begin{tabular}{ccccc}
      n & w & w & se & e \\
      n & w & n & s & e \\
      n & w & nw & e & e \\
      n & w & n & e & w \\
      n & w & n & e & w \\
    \end{tabular}

    \item{}[7 pts] Implement value iteration algorithm in the following
    function to find the optimal policy.
    \begin{minted}{python}
def value_iter(P, theta=0.0001, gamma=0.9):
    return V, policy
    \end{minted}
    Other than the code, please put your value function with 2 decimals
    in your solution. And also visualize the optimal policy.

    \item{}[7 pts] Next we change the setup of the gridworld to an episodic task.
    Whenever the agent moves to \texttt{A'} or \texttt{B'}, the episode ends.
    Write a function to compute the new transition probabilities.
    \begin{minted}{python}
def gridworld_ep(slip_prob=0.2):
    return P
    \end{minted}
    Under the episodic setup, our intention is to teach the agent to get the
    maximum reward 10 with fewest steps from all states from which it is
    feasible to get the maximum reward. Run your \texttt{value\_iter} with
    \texttt{gamma=1}. Based on the optimal value function and optimal policy returned by your
    function, is the intention achieved?
    By setting \texttt{gamma} to 0.8 and 0.9, can we achieve the intention?
    Put your value function for \texttt{gamma=0.8} with 2 decimals in your
    solution.

    \item{}[7 pts] What is the critical value of \texttt{gamma} that changes the agent's
    behavior from always achieving our intention to failure somewhere? 
\end{enumerate}
