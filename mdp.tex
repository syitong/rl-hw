%!TEX root = ./homework1.tex

\paragraph{Domain Definition}
(Variant of Example 3.5 on p60) Consider the 5 by 5 gridworld representation of
a simple finite MDP. At each cell, four actions are possible: north, south, east
and west. When the agent takes an action, there is a probability of 0.2 for it
to slip and thus leaving its location unchanged, otherwise it moves in the correct
direction by one cell. Any actions that would take the agent off the grid
also leave the agent's location unchanged and result in a reward of -1.
Other actions result in a reward of 0, except those that move the agent out
of the special state A and B. From state A, all four actions yield a reward of
+10 and take the agent to A' deterministically. From state B, all four actions
yield a reward of +5 and take the agent to B' deterministically.
\begin{center}
    \begin{tabular}{ccccc}
      o & A  & o & B  & o\\
      o & o  & o & o  & o\\
      o & o  & o & B' & o\\
      o & o  & o & o  & o\\
      o & A' & o & o  & o\\
    \end{tabular}
\end{center}
The 25 states in the grid will be identified as the integers
from $ 0 $ to $ 24 $ in the order of left to right and top to bottom.
The four actions are encoded by north=0, east=1, south=2, west=3.

Now please complete the programming problems below. Always comply with
the following rules!!
\begin{itemize}
  \item Development environment: \texttt{python 3.5+}.
  \item Allowed non-standard packages: \texttt{numpy}.
  \item Your functions must follow the definitions given in the
  problem with correct default values passed.
  \item Put all your functions in one \texttt{.py} file named by your UM
  uniquename. Make sure it can be imported and all the functions can be
  called.
\end{itemize}

\begin{enumerate}
    \item Work out the transition probability and rewards for the MDP described
    above using the function defined as:
    \begin{minted}{python}
def gridworld(slip_prob=0.2):
    # slip_prob is the probability the agent slips.
    return P
    '''
    P = {
            s1: {a1: [(p(s'_1|s1,a1), s'_1, reward(s'_1,s1,a1)),
                      (p(s'_2|s1,a1), s'_2, reward(s'_2,s1,a1)),
                      ...
                     ],
                 a2: ...,
                 ...
                 },
            s2: ...,
            ...
        }
    '''
    \end{minted}
    Only non-zero transition probabilities need to be considered. In other
    words, \texttt{len(P[s][a])} is at most 4.

    \item Assume that the agent follows the uniform policy, that is,
    taking one of four actions with equal probabilities at any state.
    Implement the policy evaluation algorithm and compute the value
    function of the uniform policy.
    The function should be defined as below:
    \begin{minted}{python}
def policy_eval(P, policy=uniform_policy, theta=0.0001, gamma=0.9):
    return V
    '''
        P: as returned by your gridworld(slip=0.2).
        policy: probability distribution over actions for each states.
          Default to uniform policy.
        theta: stopping condition.
        gamma: the discount factor.
        V: 5 by 5 numpy array where each entry is the value of the
          corresponding location. Initialize V with zeros.
    '''
    \end{minted}

    \item Now implement the policy iteration algorithm in the following
    function to find the optimal policy.
    \begin{minted}{python}
def policy_iter(P, theta=0.0001, gamma=0.9):
    return V, policy
    '''
        policy: 25 by 4 numpy array where each row is a probability
          distribution over moves for a state. If it is
          deterministic, then the probability will be a one hot vector.
          If there is a tie between two actions, break the tie with
          equal probabilities.
          Initialize the policy with zeros.
    '''
    \end{minted}

    \item Implement value iteration algorithm in the following
    function to find the optimal policy.
    \begin{minted}{python}
def value_iter(P, theta=0.0001, gamma=0.9):
    return V, policy
    \end{minted}

    \item Now we change the setup of the gridworld to an episodic task.
    Whenever the agent moves to \texttt{A'} or \texttt{B'}, the game ends.
    Write a function to compute the new transition probabilities.
    \begin{minted}{python}
def gridworld_ep(slip_prob=0.2):
    return P
    \end{minted}

    \item Under the episodic setup, our goal is to teach the agent to get the
    maximum reward 10 with fewest steps. Now run your \texttt{value\_iter} with
    \texttt{gamma=1}. Based on the optimal value function returned by your
    function, can we achieve the goal?
    By setting \texttt{gamma} to 0.8 and 0.9, can we achieve the goal?

    \item What is the critical value of \texttt{gamma} that varies the behavior of
    the agent? Support your answer with math.
\end{enumerate}
